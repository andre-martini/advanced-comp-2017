{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Work on this before the next lecture on 24 April. We will talk about questions, comments, and solutions during the exercise after the third lecture.\n",
    "\n",
    "Please do form study groups! When you do, make sure you can explain everything in your own words, do not simply copy&paste from others.\n",
    "\n",
    "The solutions to a lot of these problems can probably be found with Google. Please don't. You will not learn a lot by copy&pasting from the internet.\n",
    "\n",
    "If you want to get credit/examination on this course please upload your work to **your GitHub repository** for this course **before** the next lecture starts and post a link to your repository [in this thread](https://github.com/wildtreetech/advanced-comp-2017/issues/3). If you worked on things together with others please add their names to the notebook so we can see who formed groups.\n",
    "\n",
    "---\n",
    "\n",
    "These are some useful default imports for plotting and [`numpy`](http://www.numpy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Correlation between trees. This question is about investigating the correlation between decision trees and how this effects an ensemble constructed from them. There are three methods\n",
    "for adding randomisation to the tree growing process:\n",
    "\n",
    "1. grow each tree on a bootstrap sample\n",
    "2. for each tree select a subset of features at random\n",
    "3. pick the best random split point\n",
    "\n",
    "You can use `RandomForestClassifier`, `BaggingClassifier`, and `ExtraTreesClassifier` to achieve various different sets of the above three strategies.\n",
    "\n",
    "Show how the average amount of correlation between the trees in the ensemble varies as a function of bootstrap yes/no, number of `max_features`, and picking the best split point at random or not.\n",
    "\n",
    "Pick one of the classification datasets from http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def corr_from_trees(classifier, X_train, y_train, X_test):\n",
    "    clf_fit = classifier.fit(X_train, y_train)\n",
    "    num_est = len(clf_fit.estimators_)\n",
    "    siz = int(num_est*(num_est-1)/2)\n",
    "    corr_array = np.zeros(siz)\n",
    "    for i in range(num_est):\n",
    "        # Calculate the prediction from the i-th tree of the forest\n",
    "        pr_i = clf_fit.estimators_[i].predict(X_test)\n",
    "        for j in range(i+1,num_est):\n",
    "            pr_j = clf_fit.estimators_[j].predict(X_test)\n",
    "            # Calculate the Pearson correlation coefficient between the predictions of trees i and j\n",
    "            indx_corr_array_i_j = k = int((num_est*(num_est-1)/2) - (num_est-i)*((num_est-i)-1)/2 + j - i - 1)\n",
    "            corr_array[indx_corr_array_i_j] = pearsonr(pr_i, pr_j)[0]\n",
    "            \n",
    "    return corr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset and split it in training and test set\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# unused\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RandomForestClassifier each of the \"n_estimators\" decision trees is constructed using a subsample of the training set taken with replacement (1.). Moreover, only a subset of features (selected at random) is used to construct each tree (2.).\n",
    "\n",
    "RandomForestClassifier then provide the predicted class based on the averaging of the prediction of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_range = range(1,X.shape[1])\n",
    "\n",
    "list_mean_corr_rfc = []\n",
    "list_std_corr_rfc = []\n",
    "list_mean_corr_rfc_no_bootstrap = []\n",
    "list_std_corr_rfc_no_bootstrap = []\n",
    "\n",
    "for mf in param_range:\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators = 500, n_jobs=-1, max_features=mf)\n",
    "    rfc_no_bootstrap = RandomForestClassifier(n_estimators = 500, n_jobs=-1, bootstrap=False, max_features=mf)\n",
    "\n",
    "    corr_rfc = corr_from_trees(rfc, X_train, y_train, X_test)\n",
    "    corr_rfc_no_bootstrap = corr_from_trees(rfc_no_bootstrap, X_train, y_train, X_test)\n",
    "\n",
    "    mean_corr_rfc = np.mean(corr_rfc, axis=0)\n",
    "    std_corr_rfc = np.std(corr_rfc, axis=0)\n",
    "    mean_corr_rfc_no_bootstrap = np.mean(corr_rfc_no_bootstrap, axis=0)\n",
    "    std_corr_rfc_no_bootstrap = np.std(corr_rfc_no_bootstrap, axis=0)\n",
    "    \n",
    "    list_mean_corr_rfc.append(mean_corr_rfc)\n",
    "    list_std_corr_rfc.append(std_corr_rfc)\n",
    "    list_mean_corr_rfc_no_bootstrap.append(mean_corr_rfc_no_bootstrap)\n",
    "    list_std_corr_rfc_no_bootstrap.append(std_corr_rfc_no_bootstrap)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "plt.plot(param_range, list_mean_corr_rfc, label='Bootstrap True', lw=4, color='forestgreen')\n",
    "plt.plot(param_range, list_mean_corr_rfc + list_std_corr_rfc, param_range, list_mean_corr_rfc - list_std_corr_rfc, label='Bootstrap True', alpha=0.5, ls=\"--\", color='forestgreen')\n",
    "plt.plot(param_range, list_mean_corr_rfc_no_bootstrap, label='Bootstrap False', lw=4, color='darkviolet')\n",
    "plt.plot(param_range, list_mean_corr_rfc_no_bootstrap + list_std_corr_rfc_no_bootstrap, param_range, list_mean_corr_rfc_no_bootstrap - list_std_corr_rfc_no_bootstrap, label='Bootstrap False', alpha=0.5, ls=\"--\", color='darkviolet')\n",
    "\n",
    "plt.xlabel(\"Max features\")\n",
    "plt.xlabel(\"matthews_corrcoef\")\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n",
    "# X, y = load_wine()\n",
    "\n",
    "# clf = RandomForestClassifier()\n",
    "# clf.fit(X[3:], y[3:])\n",
    "\n",
    "# for n, tree in enumerate(clf.estimators_):\n",
    "#     print(\"Prediction by tree %i\" % n, tree.predict(X[:3,:]))\n",
    "    \n",
    "# print(\"Prediction by forest\", clf.predict(X[:3,:]))\n",
    "\n",
    "# clf.classes_[2]\n",
    "\n",
    "##### RandomForestRegressor\n",
    "\n",
    "# param_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "# #XXX=\"\"\"\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# _, test_scores = validation_curve(RandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "#                                   X, y, cv=cv,\n",
    "#                                   param_name='max_features', param_range=param_range,\n",
    "#                                   scoring='neg_mean_squared_error')\n",
    "# test_scores = -np.mean(test_scores, axis=1)\n",
    "# plt.plot(param_range, test_scores, label='RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Compare the feature importances calculated by a `RandomForestClassifier`, `ExtraTreesClassifier` and `GradientBoostedTreesClassifier` on the digits dataset. You might have to tune `n_estimators` to get good performance. Which parts of the images is the most important and do you agree with the interpretation of the classifiers? (Bonus) Do the importances change if you change to problem to be a classification problem of odd vs even digit?\n",
    "\n",
    "You can load the data set with: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Question 3\n",
    "\n",
    "This is a regression problem. Use a gradient boosted tree regressor (tune the `max_depth`, `learning_rate` and `n_estimators` parameters) to study the importance of the different features as well as the partial dependence of the output on individual features as well as pairs of features.\n",
    "\n",
    "* can you identify uninformative features?\n",
    "* how do the interactions between the features show up in the partial dependence plots?\n",
    "\n",
    "(Help: `rgr = GradientBoostingRegressor(n_estimators=200, max_depth=2, learning_rate=0.1)\n",
    "` seems to work quite well)\n",
    "(Help: to produce 1D and 2D partial dependence plots pass `[0,1, (0,1)]` as the `features` argument of `plot_partial_dependence`. More details in the function's documentation.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "def make_data(n_samples=800, n_features=8, noise=0.2, random_state=2):\n",
    "    generator = check_random_state(random_state)\n",
    "\n",
    "    X = generator.rand(n_samples, n_features)\n",
    "    y = 10 * (X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 \\\n",
    "        + 10 * X[:, 3] + 10 * X[:, 4] + noise * generator.randn(n_samples)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X,y = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## (Bonus) Question 4\n",
    "\n",
    "House prices in California. Use a gradient boosted regression tree model to build a model that can predict house prices in California (`GradientBoostingRegressor` is your friend).\n",
    "\n",
    "Plot each of the features as a scatter plot with the target to learn about each variable. You can also make a plot of two features and use the target as colour.\n",
    "\n",
    "Fit a model and tune the model complexity using a training and test data set.\n",
    "\n",
    "Explore the feature importances and partial dependences that are important to the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "# if the above doesn't work, download `cal_housing_py3.pkl` from the GitHub repository\n",
    "# and adjust the path to the downloaded file which is passed to `load()`\n",
    "# uncomment the following lines\n",
    "#from sklearn.externals.joblib import load\n",
    "#d = load('/home/username/Downloads/cal_housing_py3.pkz')\n",
    "#X, y = d[:,1:], d[:,0]/100000\n",
    "#X[:, 2] /= X[:, 5]\n",
    "#X[:, 3] /= X[:, 5]\n",
    "#X[:, 5] = X[:, 4] / X[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
