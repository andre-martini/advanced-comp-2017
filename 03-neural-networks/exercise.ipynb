{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Work on this before the next lecture on 1 May. We will talk about questions, comments, and solutions during the exercise after the third lecture.\n",
    "\n",
    "Please do form study groups! When you do, make sure you can explain everything in your own words, do not simply copy&paste from others.\n",
    "\n",
    "The solutions to a lot of these problems can probably be found with Google. Please don't. You will not learn a lot by copy&pasting from the internet.\n",
    "\n",
    "If you want to get credit/examination on this course please upload your work to **your GitHub repository** for this course **before** the next lecture starts and post a link to your repository [in this thread](). If you worked on things together with others please add their names to the notebook so we can see who formed groups.\n",
    "\n",
    "---\n",
    "\n",
    "These are some useful default imports for plotting and [`numpy`](http://www.numpy.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding analytic gradients of expressions. Use the backprop framework we built in the lecture to compute the analytic gradient of an expression. This is useful to get thinking about\n",
    "writing computations as graphs and to see first hand that there is no magic involved\n",
    "in automatically finding derivatives.\n",
    "\n",
    "For example, using the expression: $f(x) = \\sin(x^2)$: implement a module for $\\sin(x)$, build the graph representing the function, plot the expression as well as the gradient as a function of $x$.\n",
    "\n",
    "* add a new operation (e.g. $\\sin, \\cos, \\exp, x^y$, ...) to the `Addition` and `Multiply` modules.\n",
    "* build a new expression using the available expressions\n",
    "* plot your expression as well as its gradient\n",
    "* compare the gradient to one you worked out by hand (or some other software package)\n",
    "\n",
    "ps. if you want to use automatic differentiation for serious projects outside of neural\n",
    "network projects checkout packages like https://github.com/HIPS/autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Multiply:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y\n",
    "    \n",
    "    def backward(self, dLdz):\n",
    "        dzdx = self.y\n",
    "        dLdx = dLdz * dzdx\n",
    "        \n",
    "        dzdy = self.x\n",
    "        dLdy = dLdz * dzdy\n",
    "        return [dLdx, dLdy]\n",
    "\n",
    "\n",
    "class Add:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x + y\n",
    "        \n",
    "    def backward(self, dLdz):\n",
    "        dzdy = 1\n",
    "        dzdx = 1\n",
    "        return [dLdz * dzdy, dLdz * dzdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_with_gradients(x, y, z):\n",
    "    q = Add()\n",
    "    f = Multiply()\n",
    "    \n",
    "    q_out = q.forward(x, y)\n",
    "    f_out = f.forward(q_out, z)\n",
    "    \n",
    "    grad_f = f.backward(1.)\n",
    "    grad_q = q.backward(grad_f[0])\n",
    "\n",
    "    gradients = [grad_q[0], grad_q[1], grad_f[1]]\n",
    "    return f_out, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Use the circle data set from exercise 1 and build a neural network classifier that can solve the problem (scikit-learn provides a MLPClassifier classifier that implements a neural network). Comment on:\n",
    "\n",
    "* what is the minimum number of layers\n",
    "* what is the minimum width of each layer\n",
    "* does the answer change if you provide polynomial features?\n",
    "* thinking about how NNs distort and transform the problem space, can you create\n",
    "  a visualisation of what it is the network is doing to make this problem linearly\n",
    "  separable?\n",
    "* is there a difference between using the tanh and ReLU activation functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2.5\n",
    "\n",
    "Use the spiral data set build a neural network classifier that can solve the problem (scikit-learn provides a MLPClassifier classifier that implements a neural network). Comment on:\n",
    "* what is the minimum number of layers\n",
    "* what is the minimum width of each layer\n",
    "* is there a difference between using the tanh and ReLU activation functions?\n",
    "* (bonus) build the same neural network using `keras` instead.\n",
    "* (bonus) can you create a visualisation to show how the network transforms the problem? (Tim isn't sure this can be done, so don't spend forever on this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_spiral():\n",
    "    N = 100 # number of points per class\n",
    "    K = 3 # number of classes\n",
    "    X = np.zeros((N*K, 2)) # data matrix (each row = single example)\n",
    "    y = np.zeros(N*K, dtype='uint8') # class labels\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        r = np.linspace(0.0, 1, N) # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3\n",
    "\n",
    "Build a fully connected neural network and a ConvNet to classify hand written digits.\n",
    "\n",
    "scikit-learn's NN implementation does not support convolutional layers so it is probably\n",
    "best to implement both using `keras`.\n",
    "\n",
    "Experiment with different network architectures, different optimisers and such.\n",
    "\n",
    "You should be able to achieve accuracies > 95% pretty quickly. With a bit of experimenting\n",
    "you can probably reach 98% or 99%.\n",
    "\n",
    "This question is about getting you comfortable using `keras` and constructing networks\n",
    "from basic building blocks. Feel free to experiment, brnach out and try different things.\n",
    "\n",
    "Note: this might take a while to train, unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading the MNIST (or digits) dataset in keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# the first time you run this you need to have internet so it can download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the layers we discussed in class, checkout the documentation if you need more\n",
    "# or want to experiment\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# your solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
